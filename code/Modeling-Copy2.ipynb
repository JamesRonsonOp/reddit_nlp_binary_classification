{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Sci-kit libraries\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('../data/wine_beer_concatenated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resources for a newbie home winemaker I want t...</td>\n",
       "      <td>winemaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A question about kit wine I’ve made quite a fe...</td>\n",
       "      <td>winemaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Riesling - My First Wine Hi Everyone,\\n\\nI am ...</td>\n",
       "      <td>winemaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Persimmon Wine Straining? Hello! This is my fi...</td>\n",
       "      <td>winemaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wire used for Trellis Hey all - I'm planting s...</td>\n",
       "      <td>winemaking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            selftext   subreddit\n",
       "0  Resources for a newbie home winemaker I want t...  winemaking\n",
       "1  A question about kit wine I’ve made quite a fe...  winemaking\n",
       "2  Riesling - My First Wine Hi Everyone,\\n\\nI am ...  winemaking\n",
       "3  Persimmon Wine Straining? Hello! This is my fi...  winemaking\n",
       "4  Wire used for Trellis Hey all - I'm planting s...  winemaking"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4297</th>\n",
       "      <td>Any input on this recipe 6 lb - Pale Malt, Mar...</td>\n",
       "      <td>Homebrewing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>Bottling Tepache Hello brewers, \\n\\na couple d...</td>\n",
       "      <td>Homebrewing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>Wiring a spa panel for ebiab gfi I've been all...</td>\n",
       "      <td>Homebrewing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>What can I ferment at 60-65°F? The basement in...</td>\n",
       "      <td>Homebrewing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4301</th>\n",
       "      <td>Does a decoction mash with wheat leave a bread...</td>\n",
       "      <td>Homebrewing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               selftext    subreddit\n",
       "4297  Any input on this recipe 6 lb - Pale Malt, Mar...  Homebrewing\n",
       "4298  Bottling Tepache Hello brewers, \\n\\na couple d...  Homebrewing\n",
       "4299  Wiring a spa panel for ebiab gfi I've been all...  Homebrewing\n",
       "4300  What can I ferment at 60-65°F? The basement in...  Homebrewing\n",
       "4301  Does a decoction mash with wheat leave a bread...  Homebrewing"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Homebrewing    2367\n",
       "winemaking     1935\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizing subreddit column for targeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['subreddit'] = posts['subreddit'].map({'winemaking': 0, 'Homebrewing': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2367\n",
       "0    1935\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "**Interpretation**\n",
    "* If we guessed at random we would select Homebrewing subreddit 55% of the time and winemaking subreddit 45% of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.550209\n",
       "0    0.449791\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target and Predictor Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = posts['selftext']\n",
    "y = posts['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test/Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, \n",
    "                                                   random_state = 42, \n",
    "                                                   stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create loop to loop through vectorizers, create a pipeline and loop through potential models while grid search looks for optimal hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on setting voting classifier parameter to 'soft' or 'hard'. https://towardsdatascience.com/how-voting-classifiers-work-f1c8e41d30ff\n",
    "\n",
    "Soft on even number of classifiers. Hard on odd number of classifiers. Since I am using 4 classifiers I will set it to soft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n",
      "Best score for CountVectorizer is: \n",
      "    0.9173\n",
      "    \n",
      "{'clf': VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
      "                             ('rf', RandomForestClassifier()),\n",
      "                             ('mnb', MultinomialNB()), ('svc', SVC())]), 'vect__max_df': 0.9, 'vect__max_features': 500, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english'}\n",
      "\n",
      "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n",
      "Best score for Tf-IDF Vectorizer is: \n",
      "    0.917\n",
      "    \n",
      "{'clf': RandomForestClassifier(min_samples_split=5, n_estimators=50), 'vect__max_df': 0.9, 'vect__max_features': 500, 'vect__ngram_range': (1, 3), 'vect__stop_words': 'english'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## creating a list of our two chosen vectorizers to iterate through in our grid search\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer()]\n",
    "\n",
    "## creating variables to accept tuning parameters\n",
    "max_feat = [300, 500]  \n",
    "ngram_range = [(1, 3), (1, 2)] \n",
    "stop_words = [None, 'english'] \n",
    "max_df = [0.9, 0.8] \n",
    "\n",
    "## creating any empty results list to capture our cv_results_ at the end of each iteration\n",
    "results = []\n",
    "\n",
    "## looping through both vectorizers\n",
    "for vect in vectorizer:\n",
    "    \n",
    "    ## creating a pipeline for our vectorizer and classifier models\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', vect),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    ## generating our parameters for vectorizers\n",
    "    vect_params = {'vect__max_features': max_feat,\n",
    "                    'vect__stop_words': stop_words,\n",
    "                    'vect__ngram_range': ngram_range,\n",
    "                    'vect__max_df': max_df\n",
    "                  }\n",
    "    parameters = [\n",
    "        {\n",
    "            ## Logistic Regression\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (LogisticRegression(solver='liblinear'), ), ## setting our first classifier model\n",
    "            'clf__penalty': ('l1', 'l2'), #2\n",
    "            'clf__C': (.5, 1.0), #4 288*2\n",
    "        }, \n",
    "        {\n",
    "            ## Multinomial Bayes\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (MultinomialNB(), ),  ## setting our second classifier model\n",
    "            'clf__alpha': (.5, 1.0)  #2 72*2\n",
    "        },\n",
    "        {\n",
    "            ## SVC\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (SVC(gamma='scale', ), ),\n",
    "            'clf__kernel': ('rbf', 'poly') \n",
    "        },\n",
    "        {\n",
    "            ## RandomForestClassifier\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (RandomForestClassifier(n_estimators=50, min_samples_split=5), ),\n",
    "        },\n",
    "        {\n",
    "            ## putting together an ensemble model\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (VotingClassifier(estimators=[('lr', LogisticRegression()), \n",
    "                                                 ('rf', RandomForestClassifier()), \n",
    "                                                 ('mnb', MultinomialNB()), \n",
    "                                                 ('svc', SVC())],                                           \n",
    "                                            voting='hard'), )\n",
    "        }\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    ## performing our grid search with the inherited pipeline and parameters\n",
    "    grid_search = GridSearchCV(pipeline, \n",
    "                               parameters,\n",
    "                               cv=3,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=1,\n",
    "                               return_train_score=True\n",
    "                              )\n",
    "    \n",
    "    ## running an if statement to print the type of vectorizer used\n",
    "    if vect == vectorizer[0]:\n",
    "        vect_string = \"CountVectorizer\"\n",
    "    \n",
    "    else:\n",
    "        vect_string = \"Tf-IDF Vectorizer\"\n",
    "    \n",
    "    ## fitting our model and printing our best scores and parameters\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'''Best score for {vect_string} is: \n",
    "    {round(grid_search.best_score_, 4)}\n",
    "    ''')\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"\")\n",
    "    \n",
    "    ## appending our cv_results_ to the end of results\n",
    "    results.append(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__penalty</th>\n",
       "      <th>param_vect__max_df</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>...</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.633313</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.486775</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886454</td>\n",
       "      <td>0.886341</td>\n",
       "      <td>0.886748</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>138</td>\n",
       "      <td>0.891380</td>\n",
       "      <td>0.884903</td>\n",
       "      <td>0.895916</td>\n",
       "      <td>0.890733</td>\n",
       "      <td>0.004519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.359614</td>\n",
       "      <td>0.121083</td>\n",
       "      <td>0.430986</td>\n",
       "      <td>0.145028</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904382</td>\n",
       "      <td>0.893320</td>\n",
       "      <td>0.897043</td>\n",
       "      <td>0.005190</td>\n",
       "      <td>86</td>\n",
       "      <td>0.901345</td>\n",
       "      <td>0.899851</td>\n",
       "      <td>0.900896</td>\n",
       "      <td>0.900697</td>\n",
       "      <td>0.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.363829</td>\n",
       "      <td>0.124363</td>\n",
       "      <td>0.471265</td>\n",
       "      <td>0.077995</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887450</td>\n",
       "      <td>0.887338</td>\n",
       "      <td>0.887413</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>136</td>\n",
       "      <td>0.891380</td>\n",
       "      <td>0.884903</td>\n",
       "      <td>0.895916</td>\n",
       "      <td>0.890733</td>\n",
       "      <td>0.004519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852647</td>\n",
       "      <td>0.052690</td>\n",
       "      <td>0.264987</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904382</td>\n",
       "      <td>0.893320</td>\n",
       "      <td>0.897043</td>\n",
       "      <td>0.005190</td>\n",
       "      <td>86</td>\n",
       "      <td>0.901345</td>\n",
       "      <td>0.900349</td>\n",
       "      <td>0.900896</td>\n",
       "      <td>0.900863</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.749457</td>\n",
       "      <td>0.040503</td>\n",
       "      <td>0.473530</td>\n",
       "      <td>0.023937</td>\n",
       "      <td>LogisticRegression(solver='liblinear')</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892430</td>\n",
       "      <td>0.885344</td>\n",
       "      <td>0.888408</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>132</td>\n",
       "      <td>0.891380</td>\n",
       "      <td>0.892875</td>\n",
       "      <td>0.894422</td>\n",
       "      <td>0.892892</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2.403066</td>\n",
       "      <td>0.032255</td>\n",
       "      <td>0.556340</td>\n",
       "      <td>0.042589</td>\n",
       "      <td>VotingClassifier(estimators=[('lr', LogisticRe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918327</td>\n",
       "      <td>0.908275</td>\n",
       "      <td>0.910328</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>16</td>\n",
       "      <td>0.975087</td>\n",
       "      <td>0.975087</td>\n",
       "      <td>0.972610</td>\n",
       "      <td>0.974261</td>\n",
       "      <td>0.001168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>4.913290</td>\n",
       "      <td>0.158064</td>\n",
       "      <td>1.318622</td>\n",
       "      <td>0.035868</td>\n",
       "      <td>VotingClassifier(estimators=[('lr', LogisticRe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913347</td>\n",
       "      <td>0.906281</td>\n",
       "      <td>0.908335</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>22</td>\n",
       "      <td>0.976582</td>\n",
       "      <td>0.978575</td>\n",
       "      <td>0.975598</td>\n",
       "      <td>0.976918</td>\n",
       "      <td>0.001239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2.945695</td>\n",
       "      <td>0.035838</td>\n",
       "      <td>0.725745</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>VotingClassifier(estimators=[('lr', LogisticRe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918327</td>\n",
       "      <td>0.920239</td>\n",
       "      <td>0.914980</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980568</td>\n",
       "      <td>0.980070</td>\n",
       "      <td>0.980578</td>\n",
       "      <td>0.980405</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>4.316001</td>\n",
       "      <td>0.211861</td>\n",
       "      <td>1.272064</td>\n",
       "      <td>0.070193</td>\n",
       "      <td>VotingClassifier(estimators=[('lr', LogisticRe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914343</td>\n",
       "      <td>0.904287</td>\n",
       "      <td>0.908667</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>21</td>\n",
       "      <td>0.976582</td>\n",
       "      <td>0.979073</td>\n",
       "      <td>0.975100</td>\n",
       "      <td>0.976918</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2.666209</td>\n",
       "      <td>0.030241</td>\n",
       "      <td>0.594277</td>\n",
       "      <td>0.110172</td>\n",
       "      <td>VotingClassifier(estimators=[('lr', LogisticRe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916335</td>\n",
       "      <td>0.919242</td>\n",
       "      <td>0.913320</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>5</td>\n",
       "      <td>0.980568</td>\n",
       "      <td>0.981066</td>\n",
       "      <td>0.981076</td>\n",
       "      <td>0.980903</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         1.633313      0.003763         0.486775        0.010979   \n",
       "1         1.359614      0.121083         0.430986        0.145028   \n",
       "2         1.363829      0.124363         0.471265        0.077995   \n",
       "3         0.852647      0.052690         0.264987        0.011226   \n",
       "4         1.749457      0.040503         0.473530        0.023937   \n",
       "..             ...           ...              ...             ...   \n",
       "155       2.403066      0.032255         0.556340        0.042589   \n",
       "156       4.913290      0.158064         1.318622        0.035868   \n",
       "157       2.945695      0.035838         0.725745        0.012690   \n",
       "158       4.316001      0.211861         1.272064        0.070193   \n",
       "159       2.666209      0.030241         0.594277        0.110172   \n",
       "\n",
       "                                             param_clf param_clf__C  \\\n",
       "0               LogisticRegression(solver='liblinear')          0.5   \n",
       "1               LogisticRegression(solver='liblinear')          0.5   \n",
       "2               LogisticRegression(solver='liblinear')          0.5   \n",
       "3               LogisticRegression(solver='liblinear')          0.5   \n",
       "4               LogisticRegression(solver='liblinear')          0.5   \n",
       "..                                                 ...          ...   \n",
       "155  VotingClassifier(estimators=[('lr', LogisticRe...          NaN   \n",
       "156  VotingClassifier(estimators=[('lr', LogisticRe...          NaN   \n",
       "157  VotingClassifier(estimators=[('lr', LogisticRe...          NaN   \n",
       "158  VotingClassifier(estimators=[('lr', LogisticRe...          NaN   \n",
       "159  VotingClassifier(estimators=[('lr', LogisticRe...          NaN   \n",
       "\n",
       "    param_clf__penalty param_vect__max_df param_vect__max_features  \\\n",
       "0                   l1                0.9                      300   \n",
       "1                   l1                0.9                      300   \n",
       "2                   l1                0.9                      300   \n",
       "3                   l1                0.9                      300   \n",
       "4                   l1                0.9                      500   \n",
       "..                 ...                ...                      ...   \n",
       "155                NaN                0.8                      300   \n",
       "156                NaN                0.8                      500   \n",
       "157                NaN                0.8                      500   \n",
       "158                NaN                0.8                      500   \n",
       "159                NaN                0.8                      500   \n",
       "\n",
       "    param_vect__ngram_range  ... split1_test_score split2_test_score  \\\n",
       "0                    (1, 3)  ...          0.886454          0.886341   \n",
       "1                    (1, 3)  ...          0.904382          0.893320   \n",
       "2                    (1, 2)  ...          0.887450          0.887338   \n",
       "3                    (1, 2)  ...          0.904382          0.893320   \n",
       "4                    (1, 3)  ...          0.892430          0.885344   \n",
       "..                      ...  ...               ...               ...   \n",
       "155                  (1, 2)  ...          0.918327          0.908275   \n",
       "156                  (1, 3)  ...          0.913347          0.906281   \n",
       "157                  (1, 3)  ...          0.918327          0.920239   \n",
       "158                  (1, 2)  ...          0.914343          0.904287   \n",
       "159                  (1, 2)  ...          0.916335          0.919242   \n",
       "\n",
       "    mean_test_score std_test_score  rank_test_score  split0_train_score  \\\n",
       "0          0.886748       0.000498              138            0.891380   \n",
       "1          0.897043       0.005190               86            0.901345   \n",
       "2          0.887413       0.000053              136            0.891380   \n",
       "3          0.897043       0.005190               86            0.901345   \n",
       "4          0.888408       0.002971              132            0.891380   \n",
       "..              ...            ...              ...                 ...   \n",
       "155        0.910328       0.005875               16            0.975087   \n",
       "156        0.908335       0.003563               22            0.976582   \n",
       "157        0.914980       0.006135                1            0.980568   \n",
       "158        0.908667       0.004206               21            0.976582   \n",
       "159        0.913320       0.006430                5            0.980568   \n",
       "\n",
       "     split1_train_score  split2_train_score  mean_train_score  std_train_score  \n",
       "0              0.884903            0.895916          0.890733         0.004519  \n",
       "1              0.899851            0.900896          0.900697         0.000626  \n",
       "2              0.884903            0.895916          0.890733         0.004519  \n",
       "3              0.900349            0.900896          0.900863         0.000407  \n",
       "4              0.892875            0.894422          0.892892         0.001242  \n",
       "..                  ...                 ...               ...              ...  \n",
       "155            0.975087            0.972610          0.974261         0.001168  \n",
       "156            0.978575            0.975598          0.976918         0.001239  \n",
       "157            0.980070            0.980578          0.980405         0.000237  \n",
       "158            0.979073            0.975100          0.976918         0.001640  \n",
       "159            0.981066            0.981076          0.980903         0.000237  \n",
       "\n",
       "[160 rows x 25 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Metrics With Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassification Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity/Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
